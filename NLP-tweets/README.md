This file uses tweet data as raw text to introduce foundational NLP and data-science techniques applied to noisy, real-world language. The notebook
treats tweets not as individual messages but as a corpus of short documents, applying preprocessing steps such as lowercasing, tokenization, and filtering
to normalize the text. It then computes unigram and bigram frequency statistics to analyze both individual word usage and patterns within the corpus.
These frequency-based representations are used to explore vocabulary distributions and common word pairings, where a small number of terms and phrases
dominate while many occur rarely. Rather than modeling semantics or making predictions, the focus here is on transforming raw text into structured
representations and analyzing statistical patterns, providing the groundwork for later NLP tasks.
